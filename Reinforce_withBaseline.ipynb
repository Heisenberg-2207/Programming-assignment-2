{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set constants for training\n",
    "seed = 543\n",
    "log_interval = 10\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,state_shape = 4,action_size = 2 ):\n",
    "        super(Network, self).__init__()\n",
    "        self.affine1 = nn.Linear(state_shape, 128)\n",
    "        self.action_head = nn.Linear(128, action_size)\n",
    "        self.saved_actions = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_states = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "        return action_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNetwork(nn.Module):\n",
    "    def __init__(self, state_size, seed, fc1_units=128, fc2_units=64):\n",
    "        super(VNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_MCWB:\n",
    "\n",
    "    def __init__(self,env):\n",
    "        env = gym.make(env)\n",
    "        self.env = env\n",
    "        seed  =543\n",
    "        self.episodes = 500\n",
    "        env.reset()\n",
    "        state_shape = env.observation_space.shape[0]\n",
    "        no_of_actions = env.action_space.n\n",
    "        policy = Network(state_shape,no_of_actions).to(device)\n",
    "        self.policy = policy\n",
    "        self.optimizerP = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "        self.vnetwork_local = VNetwork(state_shape, seed).to(device)\n",
    "        self.optimizerV = optim.Adam(self.vnetwork_local.parameters(), lr=3e-2)\n",
    "        self.vnetwork_target = VNetwork(state_shape, seed).to(device)\n",
    "        self.max_len = 10000\n",
    "\n",
    "    def learn_Value(self, states, actions, rewards, next_states, dones):\n",
    "        next_states = torch.tensor(next_states).to(device)\n",
    "        V_targets_next = self.vnetwork_target(next_states).detach()\n",
    "        V_targets =  + (gamma * V_targets_next * (1 - dones))\n",
    "        actions = torch.tensor(actions).view(-1, 1).to(device)\n",
    "        V_expected = self.vnetwork_local(torch.tensor(states).to(device))\n",
    "        loss = F.mse_loss(V_expected, V_targets)\n",
    "        self.optimizerV.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.vnetwork_local.parameters():\n",
    "              param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizerV.step()\n",
    "        \n",
    "    def select_action(self,state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        probs = self.policy(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self,rewards, states, actions):\n",
    "        G = 0\n",
    "        gamma = 0.99\n",
    "        self.optimizerP.zero_grad()\n",
    "        for i in reversed(range(len(rewards))):  \n",
    "            reward = rewards[i]\n",
    "            state = torch.tensor(states[i].reshape(1, -1),\n",
    "                                 dtype=torch.float).to(device)\n",
    "            action = torch.tensor(actions[i]).view(-1, 1).to(device)\n",
    "            log_prob = torch.log(self.policy(state)).gather(1,action)\n",
    "            #print(log_prob)\n",
    "            G = gamma * G + reward\n",
    "            advantage =  G  - self.vnetwork_local(state)\n",
    "            #print(advantage)\n",
    "            loss = -log_prob * advantage \n",
    "            loss.backward()             \n",
    "        self.optimizerP.step()\n",
    "        del self.policy.episode_rewards[:]\n",
    "        del self.policy.saved_actions[:]\n",
    "        del self.policy.episode_states[:]\n",
    "     \n",
    "    def train(self):\n",
    "        total_reward = []\n",
    "        avg_reward = []\n",
    "        running_reward = 10\n",
    "        # run infinitely many episodes\n",
    "        for i_episode in range(self.episodes):\n",
    "            # reset environment and episode reward\n",
    "            state, _ = self.env.reset()\n",
    "            ep_reward = 0\n",
    "            for t in range(1, self.max_len):\n",
    "            # select action from policy\n",
    "                self.policy.episode_states.append(state)\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.learn_Value(state, action, reward, next_state, done)\n",
    "                self.policy.episode_rewards.append(reward)\n",
    "                self.policy.saved_actions.append(action)\n",
    "                state = next_state\n",
    "                ep_reward += reward\n",
    "                if done:\n",
    "                   break \n",
    "            total_reward.append(ep_reward)\n",
    "            self.update(self.policy.episode_rewards, self.policy.episode_states, self.policy.saved_actions)   \n",
    "            running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "            avg_reward.append(running_reward)\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        return avg_reward\n",
    "    \n",
    "    def PerformExpmt(self,num_expmt):\n",
    "        reward_avgs = []\n",
    "        for i in range(num_expmt):  \n",
    "            print(\"Experiment: %d\"%(i+1))\n",
    "            rewards = self.train()   \n",
    "            reward_avgs.append(np.asarray(rewards))\n",
    "        reward_avgs_mean = np.mean(np.array(reward_avgs), axis=0)\n",
    "        reward_avgs_std = np.std(reward_avgs, axis=0)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(self.episodes), reward_avgs_mean, label='Reward Avg', color='blue')\n",
    "        plt.fill_between(range(self.episodes), reward_avgs_mean - reward_avgs_std, reward_avgs_mean + reward_avgs_std, alpha=0.3, color='blue')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.legend()\n",
    "        plt.savefig('Rewards')\n",
    "        plt.show()\n",
    "        return reward_avgs_mean, reward_avgs_std\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = REINFORCE_MCWB(\"Acrobot-v1\")\n",
    "avg_reward = reinforce.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(avg_reward)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
