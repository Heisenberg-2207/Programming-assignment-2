{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set constants for training\n",
    "seed = 543\n",
    "log_interval = 10\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,state_shape = 4,action_size = 2 ):\n",
    "        super(Network, self).__init__()\n",
    "        self.affine1 = nn.Linear(state_shape, 128)\n",
    "        self.action_head = nn.Linear(128, action_size)\n",
    "        self.saved_actions = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_states = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "        return action_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class REINFORCE_MCWB:\n",
    "\n",
    "    def __init__(self,env):\n",
    "        env = gym.make('CartPole-v1')\n",
    "        self.env = env\n",
    "        self.episodes = 500\n",
    "        env.reset()\n",
    "        state_shape = env.observation_space.shape[0]\n",
    "        no_of_actions = env.action_space.n\n",
    "        model = Network(state_shape,no_of_actions)\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        self.max_len = 10000\n",
    "        \n",
    "    def select_action(self,state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        probs = self.model(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self,rewards, states, actions):\n",
    "        G = 0\n",
    "        gamma = 0.99\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in reversed(range(len(rewards))):  \n",
    "            reward = rewards[i]\n",
    "            state = torch.tensor(states[i].reshape(1, -1),\n",
    "                                 dtype=torch.float)\n",
    "            action = torch.tensor(actions[i]).view(-1, 1)\n",
    "            log_prob = torch.log(self.model(state).gather(1, action))  \n",
    "            G = gamma * G + reward  \n",
    "            loss = -log_prob * G       \n",
    "            loss.backward()             \n",
    "        self.optimizer.step()\n",
    "        del self.model.episode_rewards[:]\n",
    "        del self.model.saved_actions[:]\n",
    "        del self.model.episode_states[:]\n",
    "     \n",
    "    def train(self):\n",
    "        total_reward = []\n",
    "        running_reward = 10\n",
    "        # run infinitely many episodes\n",
    "        for i_episode in range(self.episodes):\n",
    "            # reset environment and episode reward\n",
    "            state, _ = self.env.reset()\n",
    "            self.model.episode_states.append(state)\n",
    "            ep_reward = 0\n",
    "            for t in range(1, self.max_len):\n",
    "            # select action from policy\n",
    "                action = self.select_action(state)\n",
    "                state, reward, done, _, _ = self.env.step(action)\n",
    "                self.model.episode_states.append(state)\n",
    "                self.model.episode_rewards.append(reward)\n",
    "                self.model.saved_actions.append(action)\n",
    "                ep_reward += reward\n",
    "                if done:\n",
    "                   break \n",
    "            total_reward.append(ep_reward)\n",
    "            self.update(self.model.episode_rewards, self.model.episode_states, self.model.saved_actions)   \n",
    "            running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        return total_reward\n",
    "    \n",
    "    def PerformExpmt(self,num_expmt):\n",
    "        reward_avgs = []\n",
    "        for i in range(num_expmt):  \n",
    "            print(\"Experiment: %d\"%(i+1))\n",
    "            rewards = self.train()   \n",
    "            reward_avgs.append(np.asarray(rewards))\n",
    "        reward_avgs_mean = np.mean(np.array(reward_avgs), axis=0)\n",
    "        reward_avgs_std = np.std(reward_avgs, axis=0)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(self.episodes), reward_avgs_mean, label='Reward Avg', color='blue')\n",
    "        plt.fill_between(range(self.episodes), reward_avgs_mean - reward_avgs_std, reward_avgs_mean + reward_avgs_std, alpha=0.3, color='blue')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.legend()\n",
    "        plt.savefig('Rewards')\n",
    "        plt.show()\n",
    "        return reward_avgs_mean, reward_avgs_std\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = REINFORCE_MCWB(\"Acrobot-v1\")\n",
    "reinforce.train()\n",
    "reinforce.PerformExpmt(2)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
