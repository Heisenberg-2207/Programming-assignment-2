{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe494f9f330>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set constants for training\n",
    "seed = 543\n",
    "log_interval = 10\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "print(state_shape)\n",
    "print(no_of_actions)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self,state_shape = 4,action_size = 2 ):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(state_shape, 128)\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(128, action_size)\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_states = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "        return action_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Policy(state_shape=state_shape,action_size=no_of_actions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "max_len = 10000 # maximum number of iteration for episode to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs = model(state)\n",
    "    #print(probs)\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(m.log_prob(action))\n",
    "    # the action to take (left or right)\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode(Returns,state_number):\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    # calculate the true value using rewards returned from the environment  \n",
    "    R = torch.tensor(Returns)\n",
    "    log_prob = model.saved_actions[state_number]\n",
    "    policy_losses.append(-torch.mul(R,log_prob))\n",
    "    optimizer.zero_grad()\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum()\n",
    "    #print(loss)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "def train():\n",
    "    running_reward = 10\n",
    "    # run infinitely many episodes\n",
    "    for i_episode in range(2000):\n",
    "        # reset environment and episode reward\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        # for each episode, only run 9999 steps so that we don't\n",
    "        # infinite loop while learning\n",
    "        for t in range(1, max_len):\n",
    "            # select action from policy\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            model.episode_states.append(state)\n",
    "            model.episode_rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        for i in range(len(model.saved_actions)):\n",
    "            R = 0\n",
    "            print(len(model.saved_actions))\n",
    "            for r in model.episode_rewards[i::-1]:\n",
    "                # calculate the discounted value\n",
    "                R = r + gamma * R\n",
    "            print(i)  \n",
    "            finish_episode(Returns=R, state_number= i)\n",
    "            \n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        # check if we have \"solved\" the cart pole problem\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:266: UserWarning: Error detected in AddmmBackward0. No forward pass information available. Enable detect anomaly during forward pass for more information. (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:91.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 2]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         R \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m R\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)  \n\u001b[0;32m---> 55\u001b[0m     \u001b[43mfinish_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mReturns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m running_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m*\u001b[39m ep_reward \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;241m*\u001b[39m running_reward\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLast reward: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAverage reward: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     60\u001b[0m           i_episode, ep_reward, running_reward))\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mfinish_episode\u001b[0;34m(Returns, state_number)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(loss)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/_tensor.py?line=511'>512</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/_tensor.py?line=512'>513</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/_tensor.py?line=513'>514</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/_tensor.py?line=514'>515</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/_tensor.py?line=519'>520</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/_tensor.py?line=520'>521</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/_tensor.py?line=521'>522</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/_tensor.py?line=522'>523</a>\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/_tensor.py?line=523'>524</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=260'>261</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=262'>263</a>\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=263'>264</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=264'>265</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=265'>266</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=266'>267</a>\u001b[0m     tensors,\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=267'>268</a>\u001b[0m     grad_tensors_,\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=268'>269</a>\u001b[0m     retain_graph,\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=269'>270</a>\u001b[0m     create_graph,\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=270'>271</a>\u001b[0m     inputs,\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=271'>272</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=272'>273</a>\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/navin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=273'>274</a>\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 2]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
