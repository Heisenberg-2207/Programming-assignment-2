{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e191715d10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set constants for training\n",
    "seed = 543\n",
    "log_interval = 10\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "print(state_shape)\n",
    "print(no_of_actions)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self,state_shape = 4,action_size = 2 ):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(state_shape, 128)\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(128, action_size)\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "        return action_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Policy(state_shape=state_shape, action_size=no_of_actions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "max_len = 10000 # maximum number of iteration for episode to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_action(state):\n",
    "    # state = torch.from_numpy(state).float()\n",
    "    # probs = model(state)\n",
    "    # #print(probs)\n",
    "    # # create a categorical distribution over the list of probabilities of actions\n",
    "    # m = Categorical(probs)\n",
    "    # action = m.sample()\n",
    "    # model.saved_actions.append(m.log_prob(action))\n",
    "    # # the action to take (left or right)\n",
    "    # return action.item()\n",
    "    \n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs = model(state)\n",
    "\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    m = Categorical(probs)\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "    action = m.sample()\n",
    "\n",
    "    # save to action buffer\n",
    "    model.saved_actions.append(m.log_prob(action))\n",
    "\n",
    "    # the action to take (left or right)\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    # policy_losses = [] # list to save actor (policy) loss\n",
    "    # # calculate the true value using rewards returned from the environment  \n",
    "    # R = torch.tensor(Returns)\n",
    "    # log_prob = model.saved_actions[state_number]\n",
    "    # policy_losses.append(-torch.mul(R,log_prob))\n",
    "    # optimizer.zero_grad()\n",
    "    # # sum up all the values of policy_losses and value_losses\n",
    "    # loss = torch.stack(policy_losses).sum()\n",
    "    # #print(loss)\n",
    "    # #torch.autograd.set_detect_anomaly(True)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in model.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob), R in zip(saved_actions, returns):\n",
    "\n",
    "        # calculate actor (policy) loss\n",
    "        policy_losses.append(-log_prob * R)\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum()\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "    \n",
    "\n",
    "\n",
    "def train():\n",
    "    running_reward = 10\n",
    "\n",
    "    # run infinitely many episodes\n",
    "    for i_episode in range(2000):\n",
    "\n",
    "        # reset environment and episode reward\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        # for each episode, only run 9999 steps so that we don't\n",
    "        # infinite loop while learning\n",
    "        for t in range(1, 10000):\n",
    "\n",
    "            # select action from policy\n",
    "            action = select_action(state)\n",
    "\n",
    "            # take the action\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # update cumulative reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # perform backprop\n",
    "        finish_episode()\n",
    "\n",
    "        # log results\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "\n",
    "        # check if we have \"solved\" the cart pole problem\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 9.00\tAverage reward: 9.95\n",
      "Episode 10\tLast reward: 16.00\tAverage reward: 16.84\n",
      "Episode 20\tLast reward: 23.00\tAverage reward: 16.83\n",
      "Episode 30\tLast reward: 169.00\tAverage reward: 31.25\n",
      "Episode 40\tLast reward: 52.00\tAverage reward: 44.33\n",
      "Episode 50\tLast reward: 39.00\tAverage reward: 47.32\n",
      "Episode 60\tLast reward: 25.00\tAverage reward: 49.76\n",
      "Episode 70\tLast reward: 31.00\tAverage reward: 43.07\n",
      "Episode 80\tLast reward: 39.00\tAverage reward: 43.21\n",
      "Episode 90\tLast reward: 36.00\tAverage reward: 44.75\n",
      "Episode 100\tLast reward: 157.00\tAverage reward: 52.22\n",
      "Episode 110\tLast reward: 74.00\tAverage reward: 57.51\n",
      "Episode 120\tLast reward: 221.00\tAverage reward: 76.73\n",
      "Episode 130\tLast reward: 119.00\tAverage reward: 278.12\n",
      "Episode 140\tLast reward: 145.00\tAverage reward: 215.96\n",
      "Episode 150\tLast reward: 418.00\tAverage reward: 270.28\n",
      "Solved! Running reward is now 776.4657257782015 and the last episode runs to 9999 time steps!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
