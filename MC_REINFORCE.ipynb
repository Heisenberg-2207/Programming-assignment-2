{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x228276c5d50>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set constants for training\n",
    "seed = 543\n",
    "log_interval = 10\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "print(state_shape)\n",
    "print(no_of_actions)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self,state_shape = 4,action_size = 2 ):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(state_shape, 128)\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(128, action_size)\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_states = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "        return action_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Policy(state_shape=state_shape,action_size=no_of_actions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "max_len = 10000 # maximum number of iteration for episode to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def select_action(state):\n",
    "#     state = torch.from_numpy(state).float()\n",
    "#     probs = model(state)\n",
    "#     # create a categorical distribution over the list of probabilities of actions\n",
    "#     m = Categorical(probs)\n",
    "#     action = m.sample()\n",
    "#     model.saved_actions.append(m.log_prob(action))\n",
    "#     # the action to take (left or right)\n",
    "#     return action.item()\n",
    "\n",
    "\n",
    "# def finish_episode(Returns,state_number):\n",
    "#     policy_losses = [] # list to save actor (policy) loss\n",
    "#     # calculate the true value using rewards returned from the environment  \n",
    "#     R = torch.tensor(Returns)\n",
    "#     log_prob = model.saved_actions[state_number]\n",
    "#     policy_losses.append(-torch.mul(R,log_prob))\n",
    "#     print(policy_losses)\n",
    "#     optimizer.zero_grad()\n",
    "#     # sum up all the values of policy_losses and value_losses\n",
    "#     torch.autograd.set_detect_anomaly(True)\n",
    "#     log_prob.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "# def train():\n",
    "#     running_reward = 10\n",
    "#     # run infinitely many episodes\n",
    "#     for i_episode in range(2000):\n",
    "#         # reset environment and episode reward\n",
    "#         state, _ = env.reset()\n",
    "#         ep_reward = 0\n",
    "#         # for each episode, only run 9999 steps so that we don't\n",
    "#         # infinite loop while learning\n",
    "#         for t in range(1, max_len):\n",
    "#             # select action from policy\n",
    "#             action = select_action(state)\n",
    "#             state, reward, done, _, _ = env.step(action)\n",
    "#             model.episode_states.append(state)\n",
    "#             model.episode_rewards.append(reward)\n",
    "#             ep_reward += reward\n",
    "#             if done:\n",
    "#                 break\n",
    "            \n",
    "#         for i in range(len(model.saved_actions)):\n",
    "#             R = 0\n",
    "#             for r in model.episode_rewards[i::-1]:\n",
    "#                 # calculate the discounted value\n",
    "#                 R = r + gamma * R\n",
    "#             finish_episode(Returns=R, state_number= i)\n",
    "#         del model.saved_actions[:]\n",
    "#         del model.episode_rewards[:]\n",
    "#         del model.episode_states[:]\n",
    "            \n",
    "#         running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "#         print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "#                   i_episode, ep_reward, running_reward))\n",
    "#         # check if we have \"solved\" the cart pole problem\n",
    "#         if running_reward > env.spec.reward_threshold:\n",
    "#             print(\"Solved! Running reward is now {} and \"\n",
    "#                   \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct MC reinforce with no baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_action(state):\n",
    "#     state = torch.from_numpy(state).float()\n",
    "#     probs = model(state)\n",
    "#     m = Categorical(probs)\n",
    "#     action = m.sample()\n",
    "#     model.saved_actions.append(m.log_prob(action))\n",
    "#     return action.item()\n",
    "\n",
    "\n",
    "# def finish_episode():\n",
    "#     policy_losses = []\n",
    "#     returns = []\n",
    "#     # calculate the returns for each time step\n",
    "#     G = 0\n",
    "#     for r in model.episode_rewards[::-1]:\n",
    "#         G = gamma * G + r\n",
    "#         returns.insert(0, G)\n",
    "#     returns = torch.tensor(returns)\n",
    "#     returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "#     # calculate the loss and update the model\n",
    "#     for log_prob, R in zip(model.saved_actions, returns):\n",
    "#         policy_losses.append(-log_prob * R)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = torch.stack(policy_losses).sum()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     # clear the saved actions and rewards for the next episode\n",
    "#     del model.saved_actions[:]\n",
    "#     del model.episode_rewards[:]\n",
    "#     del model.episode_states[:]\n",
    "\n",
    "\n",
    "# def train():\n",
    "#     running_reward = 10\n",
    "#     for i_episode in range(2000):\n",
    "#         state,_ = env.reset()\n",
    "#         ep_reward = 0\n",
    "#         for t in range(1, max_len):\n",
    "#             action = select_action(state)\n",
    "#             state, reward, done, _, _ = env.step(action)\n",
    "#             model.episode_states.append(state)\n",
    "#             model.episode_rewards.append(reward)\n",
    "#             ep_reward += reward\n",
    "#             if done:\n",
    "#                 break\n",
    "#         finish_episode()\n",
    "#         running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "#         print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "#             i_episode, ep_reward, running_reward))\n",
    "#         if running_reward > env.spec.reward_threshold:\n",
    "#             print(\"Solved! Running reward is now {} and \"\n",
    "#                   \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs = model(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    policy_losses = []\n",
    "    returns = []\n",
    "    # calculate the returns for each time step\n",
    "    G = 0\n",
    "    for r in model.episode_rewards[::-1]:\n",
    "        G = gamma * G + r\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    # calculate the loss and update the model\n",
    "    for log_prob, R in zip(model.saved_actions, returns):\n",
    "        policy_losses.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # clear the saved actions and rewards for the next episode\n",
    "    del model.saved_actions[:]\n",
    "    del model.episode_rewards[:]\n",
    "    del model.episode_states[:]\n",
    "\n",
    "\n",
    "def train():\n",
    "    running_reward = 10\n",
    "    for i_episode in range(2000):\n",
    "        state,_ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, max_len):\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            model.episode_states.append(state)\n",
    "            model.episode_rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        finish_episode()\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "            i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.6867, grad_fn=<NegBackward0>)]\n",
      "[tensor(1.3261, grad_fn=<NegBackward0>)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 2]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[218], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[216], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mepisode_rewards[i::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;66;03m# calculate the discounted value\u001b[39;00m\n\u001b[0;32m     50\u001b[0m         R \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m R\n\u001b[1;32m---> 51\u001b[0m     \u001b[43mfinish_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mReturns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\u001b[38;5;241m.\u001b[39msaved_actions[:]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\u001b[38;5;241m.\u001b[39mepisode_rewards[:]\n",
      "Cell \u001b[1;32mIn[216], line 22\u001b[0m, in \u001b[0;36mfinish_episode\u001b[1;34m(Returns, state_number)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# sum up all the values of policy_losses and value_losses\u001b[39;00m\n\u001b[0;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mlog_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 2]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
